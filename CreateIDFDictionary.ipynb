{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re, string\n",
    "punc_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords.txt\", 'r') as r:\n",
    "    stops = []\n",
    "    for line in r:\n",
    "        stops += [i.strip() for i in line.split('\\t')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "d = {}\n",
    "with open(\"captions_train2014.json\") as captions:\n",
    "    d = json.load(captions)\n",
    "import pickle\n",
    "resnet = {}\n",
    "with open(\"resnet18_features_train.pkl\",mode=\"rb\") as res:\n",
    "    resnet = pickle.load(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {1: [2, 3]})\n"
     ]
    }
   ],
   "source": [
    "d_ids = defaultdict(list, {})\n",
    "d_ids[1].append(2)\n",
    "d_ids[1].append(3)\n",
    "print(d_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ids = d_ids = defaultdict(list, {})\n",
    "for image in range(len(d[\"annotations\"])):\\\n",
    "    d_ids[d[\"annotations\"][image][\"image_id\"]].append(d[\"annotations\"][image][\"caption\"])\n",
    "happy_word_list = list(d_ids.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "40017\n",
      "80035\n",
      "120056\n",
      "160081\n",
      "200098\n",
      "240118\n",
      "280140\n",
      "320161\n",
      "360177\n",
      "400195\n",
      "24354\n"
     ]
    }
   ],
   "source": [
    "def strip_punc(corpus):\n",
    "    \"\"\" Removes all punctuation from a string.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : str\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            the corpus with all punctuation removed\"\"\"\n",
    "    # substitute all punctuation marks with \"\"\n",
    "    return punc_regex.sub('', corpus)\n",
    "def to_bag(arr,k=0,stop=[]):\n",
    "    c = Counter()\n",
    "    for a in arr:\n",
    "        c += a\n",
    "    for i in stop:\n",
    "        del c[i]\n",
    "    keys, other = unzip(c.most_common(k))\n",
    "    return (keys,c)\n",
    "def to_counter(s):\n",
    "    return Counter(strip_punc(s.lower()).split())\n",
    "def unzip(pairs):\n",
    "    \"\"\"Splits list of pairs (tuples) into separate lists.\n",
    "    \n",
    "    Example: pairs = [(\"a\", 1), (\"b\", 2)] --> [\"a\", \"b\"] and [1, 2]\n",
    "    \n",
    "    This should look familiar from our review back at the beginning of week 1\n",
    "    :)\n",
    "    \"\"\"\n",
    "    return tuple(zip(*pairs))\n",
    "def to_idf(arr,stop=[]):\n",
    "    b,c = to_bag(arr,50,stop=stop)\n",
    "    k, v = unzip(c.most_common())\n",
    "    v = np.log10(len(arr)/(np.array(v)))\n",
    "    return dict(zip(*(k,v)))\n",
    "def captions_to_idf(arr):\n",
    "    \"\"\"\n",
    "    Takes in an array of captions which are the captions and returns the big idf array\n",
    "    \"\"\"\n",
    "    big_list_of_happiness = []\n",
    "    for i in (range(len(arr))):\n",
    "        for j in range(len(arr[i])):\n",
    "            big_list_of_happiness.append(to_counter(arr[i][j]))\n",
    "            if((5*i+j)%40000==0):\n",
    "                print(len(big_list_of_happiness))\n",
    "    return to_idf(big_list_of_happiness,stop=stops)\n",
    "very_cool_array = captions_to_idf(happy_word_list)\n",
    "print(len(very_cool_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.316088868597503\n",
      "1.5160595093533684\n"
     ]
    }
   ],
   "source": [
    "test = very_cool_array\n",
    "print(test[\"happiness\"])\n",
    "print(test[\"dog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24354\n"
     ]
    }
   ],
   "source": [
    "print(len(very_cool_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"IDF.pkl\", mode=\"wb\") as opened_file:\n",
    "    pickle.dump(very_cool_array, opened_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5160595093533684"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"IDF.pkl\", mode=\"rb\") as opened_file:\n",
    "    my_loaded_grades = pickle.load(opened_file)\n",
    "my_loaded_grades[\"dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
